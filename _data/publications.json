[	
	{
		"title": "Document-level Claim Extraction and Decontextualisation for Fact-Checking",
		"authors": ["Zhenyun Deng", "Michael Sejr Schlichtkrull", "Andreas Vlachos"],
		"abstract": "Selecting which claims to check is a time-consuming task for human fact-checkers, especially from documents consisting of multiple sentences and containing multiple claims. However, existing claim extraction approaches focus more on identifying and extracting claims from individual sentences, e.g., identifying whether a sentence contains a claim or the exact boundaries of the claim within a sentence. In this paper, we propose a method for document-level claim extraction for fact-checking, which aims to extract check-worthy claims from documents and decontextualise them so that they can be understood out of context. Specifically, we first recast claim extraction as extractive summarization in order to identify central sentences from documents, then rewrite them to include necessary context from the originating document through sentence decontextualisation. Evaluation with both automatic metrics and a fact-checking professional shows that our method is able to extract check-worthy claims from documents at a higher rate than previous work, while also improving evidence retrieval.",
		"venue": "ACL",
		"year": 2024,
		"link": "https://aclanthology.org/2024.acl-long.645/",
		"pdf": "https://aclanthology.org/2024.acl-long.645.pdf",
		"bibtex": "https://aclanthology.org/2024.acl-long.645.bib",
		"code": "https://github.com/Tswings/AVeriTeC-DCE"
	},
	{
		"title": "Automated Focused Feedback Generation for Scientific Writing Assistance",
		"authors": ["Eric Chamoun", "Michael Sejr Schlichtkrull", "Andreas Vlachos"],
		"abstract": "Scientific writing is a challenging task, particularly for novice researchers who often rely on feedback from experienced peers. Recent work has primarily focused on improving surface form and style rather than manuscript content. In this paper, we propose a novel task: automated focused feedback generation for scientific writing assistance. We present SWIF2T: a Scientific WrIting Focused Feedback Tool. It is designed to generate specific, actionable and coherent comments, which identify weaknesses in a scientific paper and/or propose revisions to it. Our approach consists of four components - planner, investigator, reviewer and controller - leveraging multiple Large Language Models (LLMs) to implement them. We compile a dataset of 300 peer reviews citing weaknesses in scientific papers and conduct human evaluation. The results demonstrate the superiority in specificity, reading comprehension, and overall helpfulness of SWIF2T’s feedback compared to other approaches. In our analysis, we also identified cases where automatically generated reviews were judged better than human ones, suggesting opportunities for integration of AI-generated feedback in scientific writing.",
		"venue": "Findings of ACL",
		"year": 2024,
		"link": "https://aclanthology.org/2024.findings-acl.580/",
		"pdf": "https://aclanthology.org/2024.findings-acl.580.pdf",
		"bibtex": "https://aclanthology.org/2024.findings-acl.580.bib",
		"code": "https://github.com/ericchamoun/FocusedFeedbackGeneration"
	},
	{
		"title": "Generating Media Background Checks for Automated Source Critical Reasoning",
		"authors": ["Michael Sejr Schlichtkrull"],
		"abstract": "Not everything on the internet is true. This unfortunate fact requires both humans and models to perform complex reasoning about credibility when working with retrieved information. In NLP, this problem has seen little attention. Indeed, retrieval-augmented models are not typically expected to distrust retrieved documents. Human experts overcome the challenge by gathering signals about the context, reliability, and tendency of source documents - that is, they perform source criticism. We propose a novel NLP task focused on finding and summarising such signals. We introduce a new dataset of 6,709 \"media background checks\" derived from Media Bias / Fact Check, a volunteer-run website documenting media bias. We test open-source and closed-source LLM baselines with and without retrieval on this dataset, finding that retrieval greatly improves performance. We furthermore carry out human evaluation, demonstrating that 1) media background checks are helpful for humans, and 2) media background checks are helpful for retrieval-augmented models.",
		"venue": "Findings of EMNLP",
		"year": 2024,
		"link": "https://arxiv.org/abs/2409.00781",
		"pdf": "https://arxiv.org/pdf/2409.00781",
		"bibtex": "/download/mbc.txt",
		"code": "https://github.com/MichSchli/MediaBackgroundChecks"
	},
	{
		"title": "AVeriTeC: A Dataset for Real-world Claim Verification with Evidence from the Web",
		"authors": ["Michael Sejr Schlichtkrull", "Zhijiang Guo", "Andreas Vlachos"],
		"abstract": "Existing datasets for automated fact-checking have substantial limitations, such as relying on artificial claims, lacking annotations for evidence and intermediate reasoning, or including evidence published after the claim. In this paper we introduce AVeriTeC, a new dataset of 4,568 real-world claims covering fact-checks by 50 different organizations. Each claim is annotated with question-answer pairs supported by evidence available online, as well as textual justifications explaining how the evidence combines to produce a verdict. Through a multi-round annotation process, we avoid common pitfalls including context dependence, evidence insufficiency, and temporal leakage, and reach a substantial inter-annotator agreement of kappa = 0.619 on verdicts. We develop a baseline as well as an evaluation scheme for verifying claims through question-answering against the open web.",
		"venue": "NeurIPS",
		"year": 2023,
		"link": "https://openreview.net/forum?id=fKzSz0oyaI",
		"pdf": "https://openreview.net/pdf?id=fKzSz0oyaI",
		"bibtex": "/download/averitec.txt",
		"poster": "/download/AVeriTeC_poster.pdf",
		"code": "https://github.com/MichSchli/AVeriTeC"
	},
	{
		"title": "Are Embedded Potatoes Still Vegetables? On the Limitations of WordNet Embeddings for Lexical Semantics",
		"authors": ["Xuyou Cheng", "Michael Sejr Schlichtkrull",  "Guy Emerson"],
		"abstract": "Knowledge Base Embedding (KBE) models have been widely used to encode structured information from knowledge bases, including WordNet. However, the existing literature has predominantly focused on link prediction as the evaluation task, often neglecting exploration of the models’ semantic capabilities. In this paper, we investigate the potential disconnect between the performance of KBE models of WordNet on link prediction and their ability to encode semantic information, highlighting the limitations of current evaluation protocols. Our findings reveal that some top-performing KBE models on the WN18RR benchmark exhibit subpar results on two semantic tasks and two downstream tasks. These results demonstrate the inadequacy of link prediction benchmarks for evaluating the semantic capabilities of KBE models, suggesting the need for a more targeted assessment approach.",
		"venue": "EMNLP",
		"year": 2023,
		"link": "https://aclanthology.org/2023.emnlp-main.542/",
		"pdf": "https://aclanthology.org/2023.emnlp-main.542.pdf",
		"bibtex": "/download/potatoes.txt"
	},
	{
		"title": "The Intended Uses of Automated Fact-Checking Artefacts: Why, How and Who",
		"authors": ["Michael Sejr Schlichtkrull", "Nedjma Ousidhoum", "Andreas Vlachos"],
		"abstract": "Automated fact-checking is often presented as an epistemic tool that fact-checkers, social media consumers, and other stakeholders can use to fight misinformation. Nevertheless, few papers thoroughly discuss how. We document this by analysing 100 highly-cited papers, and annotating epistemic elements related to intended use, i.e., means, ends, and stakeholders. We find that narratives leaving out some of these aspects are common, that many papers propose inconsistent means and ends, and that the feasibility of suggested strategies rarely has empirical backing. We argue that this vagueness actively hinders the technology from reaching its goals, as it encourages overclaiming, limits criticism, and prevents stakeholder feedback. Accordingly, we provide several recommendations for thinking and writing about the use of fact-checking artefacts.",
		"venue": "Findings of EMNLP",
		"year": 2023,
		"link": "https://aclanthology.org/2023.findings-emnlp.577/",
		"pdf": "https://aclanthology.org/2023.findings-emnlp.577.pdf",
		"bibtex": "/download/epistemology.txt",
		"code": "https://github.com/MichSchli/IntendedAFCUses"
	},
	{
		"title": "UniK-QA: Unified Representations of Structured and Unstructured Knowledge for Open-Domain Question Answering",
		"authors": ["Barlas Oguz", "Xilun Chen", "Vladimir Karpukhin", "Stan Peshterliev", "Dmytro Okhonko", "Michael Sejr Schlichtkrull", "Sonal Gupta", "Yashar Mehdad", "Scott Yih"],
		"abstract": "We study open-domain question answering with structured, unstructured and semi-structured knowledge sources, including text, tables, lists and knowledge bases. Departing from prior work, we propose a unifying approach that homogenizes all sources by reducing them to text and applies the retriever-reader model which has so far been limited to text sources only. Our approach greatly improves the results on knowledge-base QA tasks by 11 points, compared to latest graph-based methods. More importantly, we demonstrate that our unified knowledge (UniK-QA) model is a simple and yet effective way to combine heterogeneous sources of knowledge, advancing the state-of-the-art results on two popular question answering benchmarks, NaturalQuestions and WebQuestions, by 3.5 and 2.6 points, respectively.The code of UniK-QA is available at: https://github.com/facebookresearch/UniK-QA.",
		"venue": "Findings of NAACL",
		"year": 2022,
		"link": "https://aclanthology.org/2022.findings-naacl.115/",
		"pdf": "https://aclanthology.org/2022.findings-naacl.115.pdf",
		"bibtex": "/download/unifiedqa.txt"
	},
	{
		"title": "A Survey on Automated Fact-Checking",
		"authors": ["Zhijiang Guo",  "Michael Sejr Schlichtkrull", "Andreas Vlachos"],
		"abstract": "Fact-checking has become increasingly important due to the speed with which both information and misinformation can spread in the modern media ecosystem. Therefore, researchers have been exploring how fact-checking can be automated, using techniques based on natural language processing, machine learning, knowledge representation, and databases to automatically predict the veracity of claims. In this paper, we survey automated fact-checking stemming from natural language processing, and discuss its connections to related tasks and disciplines. In this process, we present an overview of existing datasets and models, aiming to unify the various definitions given and identify common concepts. Finally, we highlight challenges for future research.",
		"venue": "TACL",
		"year": 2022,
		"link": "https://aclanthology.org/2022.tacl-1.11/",
		"pdf": "https://aclanthology.org/2022.tacl-1.11.pdf",
		"bibtex": "/download/fc_survey.txt"
	},
	{
		"title": "NeurIPS 2020 EfficientQA Competition: Systems, Analyses and Lessons Learned",
		"authors": ["Sewon Min", "Jordan Boyd-Graber", "Chris Alberti", "Danqi Chen", "Eunsol Choi", "Michael Collins", "Kelvin Guu", "Hannaneh Hajishirzi", "Kenton Lee", "Jennimaria Palomaki", "Colin Raffel", "Adam Roberts", "Tom Kwiatkowski", "Patrick Lewis", "Yuxiang Wu", "Heinrich Küttler", "Linqing Liu", "Pasquale Minervini", "Pontus Stenetorp", "Sebastian Riedel", "Sohee Yang", "Minjoon Seo", "Gautier Izacard", "Fabio Petroni", "Lucas Hosseini", "Nicola De Cao", "Edouard Grave", "Ikuya Yamada", "Sonse Shimaoka", "Masatoshi Suzuki", "Shumpei Miyawaki", "Shun Sato", "Ryo Takahashi", "Jun Suzuki", "Martin Fajcik", "Martin Docekal", "Karel Ondrej", "Pavel Smrz", "Hao Cheng", "Yelong Shen", "Xiaodong Liu", "Pengcheng He", "Weizhu Chen", "Jianfeng Gao", "Barlas Oğuz", "Xilun Chen", "Vladimir Karpukhin", "Stan Peshterliev", "Dmytro Okhonko", "Michael Sejr Schlichtkrull", "Sonal Gupta", "Yashar Mehdad", "Wen-tau Yih"],
		"abstract": "We review the EfficientQA competition from NeurIPS 2020. The competition focused on open-domain question answering (QA), where systems take natural language questions as input and return natural language answers. The aim of the competition was to build systems that can predict correct answers while also satisfying strict on-disk memory budgets. These memory budgets were designed to encourage contestants to explore the trade-off between storing large, redundant, retrieval corpora or the parameters of large learned models. In this report, we describe the motivation and organization of the competition, review the best submissions, and analyze system predictions to inform a discussion of evaluation for open-domain QA.",
		"venue": "NeurIPS 2020 Competitions and Demonstrations Track",
		"year": 2021,
		"link": "https://arxiv.org/abs/2101.00133",
		"pdf": "https://arxiv.org/pdf/2101.00133.pdf",
		"bibtex": "/download/efficientqa.txt"
	},
	{
		"title": "Interpreting Graph Neural Networks for NLP With Differentiable Edge Masking",
		"authors": ["Michael Sejr Schlichtkrull", "Nicola De Cao", "Ivan Titov"],
		"abstract": "Graph neural networks (GNNs) have become a popular approach to integrating structural inductive biases into NLP models. However, there has been little work on interpreting them, and specifically on understanding which parts of the graphs (e.g. syntactic trees or co-reference structures) contribute to a prediction. In this work, we introduce a post-hoc method for interpreting the predictions of GNNs which identifies unnecessary edges. Given a trained GNN model, we learn a simple classifier that, for every edge in every layer, predicts if that edge can be dropped. We demonstrate that such a classifier can be trained in a fully differentiable fashion, employing stochastic gates and encouraging sparsity through the expected L<sub>0</sub> norm. We use our technique as an attribution method to analyze GNN models for two tasks -- question answering and semantic role labeling -- providing insights into the information flow in these models. We show that we can drop a large proportion of edges without deteriorating the performance of the model, while we can analyse the remaining edges for interpreting model predictions.",
		"venue": "ICLR",
		"year": 2021,
		"link": "https://openreview.net/forum?id=WznmQa42ZAx",
		"pdf": "https://openreview.net/pdf?id=WznmQa42ZAx",
		"bibtex": "/download/graphmask_bibtex.txt",
		"poster": "/download/GraphMask_poster.pdf",
		"code": "https://github.com/MichSchli/GraphMask"
	},
	{
		"title": "Joint Verification and Reranking for Open Fact Checking Over Tables",
		"authors": ["Michael Sejr Schlichtkrull", "Vladimir Karpukhin", "Barlas Oğuz", "Mike Lewis", "Wen-tau Yih", "Sebastian Riedel"],
		"abstract": "Structured information is an important knowledge source for automatic verification of factual claims. Nevertheless, the majority of existing research into this task has focused on textual data, and the few recent inquiries into structured data have been for the closed-domain setting where appropriate evidence for each claim is assumed to have already been retrieved. In this paper, we investigate verification over structured data in the open-domain setting, introducing a joint reranking-and-verification model which fuses evidence documents in the verification component. Our open-domain model achieves performance comparable to the closed-domain state-of-the-art on the TabFact dataset, and demonstrates performance gains from the inclusion of multiple tables as well as a significant improvement over a heuristic retrieval baseline.",
		"venue": "ACL",
		"year": 2021,
		"link": "https://aclanthology.org/2021.acl-long.529/",
		"pdf": "https://aclanthology.org/2021.acl-long.529.pdf",
		"bibtex": "/download/opentables.txt"
	},
	{
		"title": "FEVEROUS: Fact Extraction and VERification Over Unstructured and Structured information",
		"authors": ["Rami Aly", "Zhijiang Guo",  "Michael Sejr Schlichtkrull", "James Thorne", "Andreas Vlachos", "Christos Christodoulopoulos", "Oana Cocarascu", "Arpit Mittal"],
		"abstract": "Fact verification has attracted a lot of attention in the machine learning and natural language processing communities, as it is one of the key methods for detecting misinformation. Existing large-scale benchmarks for this task have focused mostly on textual sources, i.e. unstructured information, and thus ignored the wealth of information available in structured formats, such as tables. In this paper we introduce a novel dataset and benchmark, Fact Extraction and VERification Over Unstructured and Structured information (FEVEROUS), which consists of 87,026 verified claims. Each claim is annotated with evidence in the form of sentences and/or cells from tables in Wikipedia, as well as a label indicating whether this evidence supports, refutes, or does not provide enough information to reach a verdict. Furthermore, we detail our efforts to track and minimize the biases present in the dataset and could be exploited by models, e.g. being able to predict the label without using evidence. Finally, we develop a baseline for verifying claims against text and tables which predicts both the correct evidence and verdict for 18% of the claims.",
		"venue": "NeurIPS 2021",
		"year": 2021,
		"link": "https://arxiv.org/abs/2106.05707",
		"pdf": "https://arxiv.org/pdf/2106.05707.pdf",
		"bibtex": "/download/feverous_bibtex.txt"
	},
	{
		"title": "Evaluating for Diversity in Question Generation over Text",
		"authors": ["Michael Sejr Schlichtkrull", "Weiwei Cheng"],
		"abstract": "Generating diverse and relevant questions over text is a task with widespread applications. We argue that commonly-used evaluation metrics such as BLEU and METEOR are not suitable for this task due to the inherent diversity of reference questions, and propose a scheme for extending conventional metrics to reflect diversity. We furthermore propose a variational encoder-decoder model for this task. We show through automatic and human evaluation that our variational model improves diversity without loss of quality, and demonstrate how our evaluation scheme reflects this improvement.",
		"link": "https://arxiv.org/abs/2008.07291",
		"pdf": "https://arxiv.org/pdf/2008.07291.pdf",
		"venue": "ArXiv",
		"year": 2020,
		"bibtex": "/download/evaluating_diversity_bibtex.txt"
	},
	{
		"title": "How do Decisions Emerge across Layers in Neural Models? Interpretation with Differentiable Masking",
		"authors": ["Nicola De Cao", "Michael Sejr Schlichtkrull", "Wilker Aziz", "Ivan Titov"],
		"abstract": "Attribution methods assess the contribution of inputs (e.g., words) to the model prediction. One way to do so is erasure: a subset of inputs is considered irrelevant if it can be removed without affecting the model prediction. Despite its conceptual simplicity, erasure is not commonly used in practice. First, the objective is generally intractable, and approximate search or leave-one-out estimates are typically used instead; both approximations may be inaccurate and remain very expensive with modern deep (e.g., BERT-based) NLP models. Second, the method is susceptible to the hindsight bias: the fact that a token can be dropped does not mean that the model `knows' it can be dropped. The resulting pruning is over-aggressive and does not reflect how the model arrives at the prediction. To deal with these two challenges, we introduce Differentiable Masking. DiffMask relies on learning sparse stochastic gates (i.e., masks) to completely mask-out subsets of the input while maintaining end-to-end differentiability. The decision to include or disregard an input token is made with a simple linear model based on intermediate hidden layers of the analyzed model. First, this makes the approach efficient at test time because we predict rather than search. Second, as with probing classifiers, this reveals what the network `knows' at the corresponding layers. This lets us not only plot attribution heatmaps but also analyze how decisions are formed across network layers. We use DiffMask to study BERT models on sentiment classification and question answering.",
		"link": "https://www.aclweb.org/anthology/2020.emnlp-main.262/",
		"pdf": "https://www.aclweb.org/anthology/2020.emnlp-main.262.pdf",
		"venue": "EMNLP",
		"year": 2020,
		"code": "https://github.com/nicola-decao/diffmask",
		"bibtex": "https://www.aclweb.org/anthology/2020.emnlp-main.262.bib"
	},
	{
		"title" : "Modeling Relational Data with Graph Convolutional Networks",
		"authors": ["Michael Sejr Schlichtkrull", "Thomas N. Kipf", "Peter Bloem", "Rianne van den Berg", "Ivan Titov", "Max Welling"],
		"abstract" : "Knowledge bases play a crucial role in many applications, for example question answering and information retrieval. Despite the great effort invested in creating and maintaining them, even the largest representatives (e.g., Yago, DBPedia or Wikidata) are highly incomplete. We introduce relational graph convolutional networks (R-GCNs) and apply them to two standard knowledge base completion tasks: link prediction (recovery of missing facts, i.e.~subject-predicate-object triples) and entity classification (recovery of missing attributes of entities). R-GCNs are a generalization of graph convolutional networks, a recent class of neural networks operating on graphs, and are developed specifically to deal with highly multi-relational data, characteristic of realistic knowledge bases. Our methods achieve competitive performance on standard benchmarks for both tasks, demonstrating especially promising results on the challenging FB15k-237 subset of Freebase.",
		"link": "https://arxiv.org/abs/1703.06103",
		"pdf": "https://arxiv.org/pdf/1703.06103.pdf",
		"venue": "ESWC",
		"year": 2018,
		"award": "Best student research paper",
                "code": "https://github.com/MichSchli/RelationPrediction",
		"bibtex": "/download/eswc2018.txt"
	},
	{
		"title": "Cross-Lingual Dependency Parsing with Late Decoding for Truly Low-Resource Languages",
		"authors": ["Michael Sejr Schlichtkrull", "Anders Søgaard"],
		"abstract": "In cross-lingual dependency annotation projection, information is often lost during  transfer because of early decoding. We present an end-to-end graph-based neural network dependency parser that can be trained to reproduce matrices of edge scores, which can be directly projected across word alignments. We show that our approach to cross-lingual dependency parsing is not only simpler, but also achieves an absolute improvement of 2.25% averaged across 10 languages compared to the previous state of the art.",
		"link": "https://www.aclweb.org/anthology/E/E17/E17-1021.pdf",
		"pdf": "https://www.aclweb.org/anthology/E/E17/E17-1021.pdf",
		"venue": "EACL",
		"year": 2017,
		"code": "https://github.com/MichSchli/Tensor-LSTM",
		"bibtex": "/download/eacl2017.txt"
	},
	{
		"title": "MSejrKu at SemEval-2016 Task 14: Taxonomy Enrichment by Evidence Ranking",
		"authors": ["Michael Sejr Schlichtkrull","Héctor Martínez Alonso"],
		"abstract": "Automatic enrichment of semantic taxonomies with novel data is a relatively unexplored task with potential benefits in a broad array of natural language processing problems. Task 14 of SemEval 2016 poses the challenge of designing systems for this task. In this paper, we describe and evaluate several machine learning systems constructed for our participation in the competition. We demonstrate an f1-score of 0.680 for our submitted systems — a small improvement over the 0.679 produced by the hard baseline.",
		"link": "https://www.aclweb.org/anthology/S/S16/S16-1209.pdf",
		"pdf": "https://www.aclweb.org/anthology/S/S16/S16-1209.pdf",
		"venue": "SemEval",
		"year": 2016,
		"bibtex": "/download/semeval2016.txt"
	},
	{
		"title": "Learning Affective Projections for Emoticons on Twitter",
		"authors": "Michael Sejr Schlichtkrull",
		"abstract": "Emoticons have in the literature been shown to modify rather than provide redundancy to the accompanying textual message. Despite this, emoticons are often used merely as labels for sentiment classification tasks. This paper aims to explore the phenomenon and discover more salient emoticon-emotion associations through an embedding-based machine learning process. Using principal component analysis and k-means clustering, it is shown how similar emoticons form groups in vector space. Furthermore, a supervised classification strategy for discovering emoticon-emotion associations is presented. A qualitative evaluation of the results shows that while the clustering is highly salient, the supervised approach does not perform as well.",
		"link": "http://ieeexplore.ieee.org/abstract/document/7390651/?section=abstract",
		"pdf": "http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7390651",
		"venue": "CogInfoCom",
		"year": 2015,
		"award":"Best paper",
		"bibtex": "/download/coginfocom2015.txt"
	}
]

